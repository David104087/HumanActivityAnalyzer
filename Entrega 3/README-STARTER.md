# ğŸš€ README-STARTER - GuÃ­a RÃ¡pida del Proyecto

## ğŸ“‹ Resumen del Proyecto

Sistema de **clasificaciÃ³n de actividades humanas en tiempo real** que usa:
- **MediaPipe Pose** para detectar landmarks corporales (33 puntos)
- **Ventanas Deslizantes (Sliding Windows)** para contexto temporal
- **Machine Learning** (Random Forest, SVM, XGBoost) para clasificar actividades

---

## ï¿½ Glosario: Â¿QuÃ© es cada cosa y por quÃ© existe?

### ï¿½ğŸ—‚ï¸ Carpetas Principales

#### `1_data_extraction/`
**Rol:** Punto de entrada del pipeline. Extrae informaciÃ³n bruta de los videos.  
**Por quÃ© existe:** Los videos MP4 no se pueden usar directamente en ML. Necesitamos convertirlos a coordenadas numÃ©ricas (landmarks).  
**QuÃ© contiene:**
- `01_extract_landmarks.py`: Lee videos frame por frame, detecta la pose humana con MediaPipe y guarda las coordenadas (x, y) de 33 puntos del cuerpo.

#### `2_feature_engineering/`
**Rol:** Transforma landmarks en caracterÃ­sticas significativas para ML.  
**Por quÃ© existe:** Las coordenadas crudas (x, y) no son buenas features porque dependen de la posiciÃ³n de la cÃ¡mara. Necesitamos medidas invariantes como Ã¡ngulos y movimiento.  
**QuÃ© contiene:**
- `02_compute_features.py`: Convierte 66 coordenadas â†’ 6 features interpretables (Ã¡ngulos de articulaciones, inclinaciÃ³n, movimiento)
- `03_create_labels_csv.py`: Herramienta auxiliar para convertir anotaciones manuales a formato estÃ¡ndar
- `04_create_window_dataset.py`: â­ **Implementa sliding windows**. Combina features de mÃºltiples frames para dar contexto temporal

#### `3_model_training/`
**Rol:** Prepara datos finales y entrena modelos de clasificaciÃ³n.  
**Por quÃ© existe:** Los datos crudos necesitan normalizaciÃ³n y divisiÃ³n. Luego entrenamos varios algoritmos para encontrar el mejor.  
**QuÃ© contiene:**
- `05_preprocess_train_split.py`: Normaliza features (StandardScaler), divide train/test, balancea clases
- `06_train_models.py`: Entrena Random Forest, SVM y XGBoost; compara sus resultados

#### `4_real_time_app/`
**Rol:** AplicaciÃ³n de producciÃ³n que usa los modelos entrenados.  
**Por quÃ© existe:** El objetivo final es clasificar actividades EN VIVO desde una webcam.  
**QuÃ© contiene:**
- `run_realtime.py`: App principal que captura video, calcula features, aplica sliding windows y predice
- `utils.py`: Funciones reutilizables para calcular Ã¡ngulos y movimiento (compartidas con entrenamiento)

#### `data/`
**Rol:** AlmacÃ©n centralizado de todos los datos del pipeline.  
**Por quÃ© existe:** Separar datos de cÃ³digo mantiene el proyecto organizado y facilita el versionado.  
**Subcarpetas:**
- `raw_videos/`: Videos originales MP4 (opcional si ya tienes landmarks)
- `preprocessed/`: Landmarks extraÃ­dos (66 columnas por frame) - Salida del paso 1
- `features_per_frame/`: Features calculados (6 columnas por frame) - Salida del paso 2
- `labels/`: Anotaciones manuales (quÃ© actividad ocurre en cada frame)
- `processed_windowed/`: Dataset final con ventanas (30 columnas) + scaler y encoder - Salida de pasos 4-5

#### `assets/`
**Rol:** Modelos y archivos necesarios para la app en tiempo real.  
**Por quÃ© existe:** Separar los modelos de producciÃ³n de los experimentales. Solo copiamos aquÃ­ el mejor modelo.  
**QuÃ© contiene:**
- `randomforest.pkl`: Modelo entrenado (copiado desde `results/models/`)
- `scaler.pkl`: StandardScaler entrenado con 30 features (âš ï¸ CRÃTICO: debe ser el mismo del entrenamiento)
- `label_encoder.pkl`: Mapeo entre nÃºmeros (0,1,2...) y nombres de actividades ("Walk", "Sit"...)

#### `results/`
**Rol:** Almacena todos los modelos entrenados y sus mÃ©tricas.  
**Por quÃ© existe:** Permite comparar mÃºltiples experimentos sin sobrescribir resultados previos.  
**Subcarpetas:**
- `models/`: Todos los modelos entrenados (.pkl)
- `metrics/`: CSVs con accuracy, precision, recall, F1-score

---

### ğŸ“„ Archivos en la RaÃ­z

#### `run_full_pipeline.py`
**Rol:** Script maestro que ejecuta TODO el pipeline automÃ¡ticamente.  
**Por quÃ© existe:** En lugar de ejecutar 6 scripts manualmente, este automatiza todo el proceso.  
**CuÃ¡ndo usarlo:** Cuando tienes nuevos datos y quieres re-entrenar desde cero.  
**QuÃ© hace:**
1. Calcula features (paso 2)
2. Crea dataset con ventanas (paso 4)
3. Preprocesa y divide datos (paso 5)
4. Entrena modelos (paso 6)
5. Copia el mejor modelo a `assets/`

#### `run_realtime.py` (tambiÃ©n en `4_real_time_app/`)
**Rol:** AplicaciÃ³n final para clasificaciÃ³n en tiempo real.  
**Por quÃ© existe:** Es el producto entregable del proyecto.  
**CuÃ¡ndo usarlo:** DespuÃ©s de entrenar, para demostrar el sistema funcionando.  
**CÃ³mo funciona:**
- Abre webcam
- Detecta pose frame por frame
- Mantiene cola de 5 frames (sliding window)
- Predice actividad cada frame
- Muestra resultado en pantalla

#### `requirements.txt`
**Rol:** Lista de todas las dependencias de Python.  
**Por quÃ© existe:** Permite replicar el entorno exacto en cualquier mÃ¡quina.  
**CuÃ¡ndo usarlo:** Primera vez que configuras el proyecto (`pip install -r requirements.txt`)

#### `utils.py` y `utils_check.py`
**Rol:** Funciones auxiliares reutilizables.  
**Por quÃ© existen:** Evitar duplicar cÃ³digo entre entrenamiento y tiempo real.  
**QuÃ© contienen:**
- `utils.py`: CÃ¡lculo de Ã¡ngulos (`calculate_angle`), inclinaciÃ³n del tronco, energÃ­a de movimiento
- `utils_check.py`: Verificaciones de sanidad (revisar formato de CSVs, etc.)

#### `README.md`
**Rol:** DocumentaciÃ³n completa y detallada del proyecto.  
**Por quÃ© existe:** GuÃ­a tÃ©cnica exhaustiva para usuarios avanzados.  
**Diferencia con README-STARTER.md:** README.md es mÃ¡s tÃ©cnico; README-STARTER.md es mÃ¡s didÃ¡ctico y visual.

---

### ğŸ“Š Archivos Clave de Datos

#### `*_preprocessed.csv` (en `data/preprocessed/`)
**Formato:** `video, frame, nx_0, nx_1, ..., nx_32, ny_0, ny_1, ..., ny_32`  
**Rol:** Landmarks crudos extraÃ­dos de MediaPipe.  
**Por quÃ© este formato:** MediaPipe detecta 33 puntos (0=nariz, 11=hombro izq, 23=cadera izq, etc.). Cada punto tiene coordenadas normalizadas (x, y) entre 0 y 1.  
**Columnas:** 2 (video, frame) + 66 (33 puntos Ã— 2 coords) = 68 columnas

#### `*_features.csv` (en `data/features_per_frame/`)
**Formato:** `video_name, frame, knee_left, knee_right, hip_left, hip_right, trunk_angle, motion_energy`  
**Rol:** Features calculados por frame (caracterÃ­sticas geomÃ©tricas).  
**Por quÃ© este formato:** Reducimos 66 nÃºmeros a 6 features significativas e invariantes a la posiciÃ³n de la cÃ¡mara.  
**Columnas:** 2 (identificadores) + 6 (features) = 8 columnas

#### `processed_labels.csv` (en `data/labels/`)
**Formato:** `video_name, frame, label`  
**Rol:** Anotaciones manuales de quÃ© actividad ocurre en cada frame.  
**Por quÃ© este formato:** Necesitamos supervisiÃ³n (ground truth) para entrenar modelos.  
**Ejemplo:**
```csv
video1,0,Walk
video1,1,Walk
video1,50,Sit
video1,51,Sit
```

#### `windowed_dataset.csv` (en `data/processed_windowed/`)
**Formato:** `f_0_knee_left, f_0_knee_right, ..., f_4_motion_energy, label`  
**Rol:** Dataset final para entrenar modelos (con contexto temporal).  
**Por quÃ© este formato:** Cada fila representa una ventana de 5 frames (30 features) + su etiqueta.  
**Columnas:** 30 (5 frames Ã— 6 features) + 1 (label) = 31 columnas  
**Por quÃ© 30 features:** El modelo necesita ver "historia" para distinguir actividades temporales.

#### `scaler.pkl`
**Rol:** Objeto StandardScaler entrenado que normaliza las 30 features.  
**Por quÃ© existe:** Los modelos de ML funcionan mejor cuando todas las features tienen media=0 y desviaciÃ³n estÃ¡ndar=1.  
**âš ï¸ CRÃTICO:** El scaler de entrenamiento DEBE ser el mismo usado en tiempo real. Si no, las predicciones serÃ¡n incorrectas.  
**CÃ³mo se usa:**
```python
# Entrenamiento
scaler.fit(X_train)  # Aprende media y std de train
X_train_scaled = scaler.transform(X_train)

# Tiempo real (usar el MISMO scaler)
X_new_scaled = scaler.transform(X_new)  # Usa media y std aprendidas
```

#### `label_encoder.pkl`
**Rol:** Mapeo bidireccional entre nombres de actividades y nÃºmeros.  
**Por quÃ© existe:** Los modelos trabajan con nÃºmeros (0, 1, 2...), pero los humanos necesitamos nombres ("Walk", "Sit"...).  
**CÃ³mo funciona:**
```python
# Entrenamiento: texto â†’ nÃºmero
["Walk", "Sit", "Walk"] â†’ [0, 1, 0]

# Tiempo real: nÃºmero â†’ texto
model.predict([...]) â†’ [0] â†’ label_encoder.inverse_transform([0]) â†’ ["Walk"]
```

---

### ğŸ¯ Archivos de Modelos

#### `randomforest.pkl`, `svm.pkl`, `xgboost.pkl`
**Rol:** Modelos entrenados listos para hacer predicciones.  
**Por quÃ© varios:** Comparamos mÃºltiples algoritmos y elegimos el mejor (usualmente Random Forest).  
**Diferencias:**
- **Random Forest**: RÃ¡pido, robusto, interpretable. Ideal para este proyecto.
- **SVM**: Preciso pero lento de entrenar. Bueno para datasets pequeÃ±os.
- **XGBoost**: Muy preciso pero complejo. Mejor para competencias.

---

## ğŸ—‚ï¸ Estructura de Carpetas (Vista RÃ¡pida)

```
Entrega 3/
â”‚
â”œâ”€â”€ 1_data_extraction/          # Paso 1: ExtracciÃ³n de landmarks
â”‚   â””â”€â”€ 01_extract_landmarks.py
â”‚
â”œâ”€â”€ 2_feature_engineering/      # Paso 2-4: Features + Ventanas
â”‚   â”œâ”€â”€ 02_compute_features.py
â”‚   â”œâ”€â”€ 03_create_labels_csv.py
â”‚   â””â”€â”€ 04_create_window_dataset.py  â­ SLIDING WINDOWS
â”‚
â”œâ”€â”€ 3_model_training/           # Paso 5-6: Preprocesado + Entrenamiento
â”‚   â”œâ”€â”€ 05_preprocess_train_split.py
â”‚   â””â”€â”€ 06_train_models.py
â”‚
â”œâ”€â”€ 4_real_time_app/            # App en tiempo real
â”‚   â”œâ”€â”€ run_realtime.py         â­ SLIDING WINDOWS en producciÃ³n
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ data/                       # Datos del pipeline
â”‚   â”œâ”€â”€ raw_videos/            # Videos originales (opcional)
â”‚   â”œâ”€â”€ preprocessed/          # Landmarks extraÃ­dos (CSV)
â”‚   â”œâ”€â”€ features_per_frame/    # Features calculados por frame
â”‚   â”œâ”€â”€ labels/                # Etiquetas manuales
â”‚   â””â”€â”€ processed_windowed/    # Dataset final con ventanas â­
â”‚
â”œâ”€â”€ assets/                     # Modelos para producciÃ³n
â”‚   â”œâ”€â”€ randomforest.pkl       # Modelo entrenado
â”‚   â”œâ”€â”€ scaler.pkl             # Escalador (30 features)
â”‚   â””â”€â”€ label_encoder.pkl      # Codificador de labels
â”‚
â”œâ”€â”€ results/                    # Resultados del entrenamiento
â”‚   â”œâ”€â”€ models/                # Modelos entrenados
â”‚   â””â”€â”€ metrics/               # MÃ©tricas y reportes
â”‚
â”œâ”€â”€ run_full_pipeline.py        # âš¡ Script maestro (ejecuta todo)
â””â”€â”€ requirements.txt
```

---

## ğŸ”¢ Orden de EjecuciÃ³n de Scripts

### Pipeline Completo (AutomÃ¡tico)
```bash
python run_full_pipeline.py
```

### Pipeline Manual (Paso a Paso)
```bash
# Paso 1: Extraer landmarks (33 puntos x,y por frame)
python 1_data_extraction/01_extract_landmarks.py

# Paso 2: Calcular 6 features por frame
python 2_feature_engineering/02_compute_features.py --batch

# Paso 3: (Opcional) Crear archivo de labels
python 2_feature_engineering/03_create_labels_csv.py

# Paso 4: Crear dataset con ventanas deslizantes â­
python 2_feature_engineering/04_create_window_dataset.py

# Paso 5: Escalar y dividir train/test
python 3_model_training/05_preprocess_train_split.py

# Paso 6: Entrenar modelos
python 3_model_training/06_train_models.py

# Paso 7: Ejecutar aplicaciÃ³n en tiempo real
python 4_real_time_app/run_realtime.py
```

---

## â­ Concepto Clave: SLIDING WINDOWS (Ventanas Deslizantes)

### Â¿Por quÃ© usar ventanas deslizantes?

Una actividad humana **NO** se puede clasificar con un solo frame. Necesitamos **contexto temporal**:
- **Caminar**: requiere ver movimiento de piernas en varios frames
- **Sentarse**: es una transiciÃ³n gradual, no instantÃ¡nea
- **Estar de pie**: necesita confirmar que no hay movimiento significativo

### ğŸ¯ Estrategia Implementada

#### 1ï¸âƒ£ Features por Frame (6 features)
Cada frame individual tiene estas caracterÃ­sticas:

| Feature | DescripciÃ³n | Rango |
|---------|-------------|-------|
| `knee_left` | Ãngulo rodilla izquierda (cadera-rodilla-tobillo) | 0Â°-180Â° |
| `knee_right` | Ãngulo rodilla derecha | 0Â°-180Â° |
| `hip_left` | Ãngulo cadera izquierda (hombro-cadera-rodilla) | 0Â°-180Â° |
| `hip_right` | Ãngulo cadera derecha | 0Â°-180Â° |
| `trunk_angle` | InclinaciÃ³n del tronco (vertical-hombros-cadera) | 0Â°-180Â° |
| `motion_energy` | EnergÃ­a de movimiento vs frame anterior | 0.0-1.0 |

#### 2ï¸âƒ£ Ventana Deslizante (WINDOW_SIZE = 5)

En lugar de clasificar con 6 features, usamos **5 frames Ã— 6 features = 30 features**:

```python
# Ejemplo visual de ventana deslizante:

Frame 0: [knee_L=150Â°, knee_R=145Â°, hip_L=170Â°, hip_R=168Â°, trunk=85Â°, motion=0.02]
Frame 1: [knee_L=148Â°, knee_R=143Â°, hip_L=169Â°, hip_R=167Â°, trunk=84Â°, motion=0.05]
Frame 2: [knee_L=146Â°, knee_R=141Â°, hip_L=168Â°, hip_R=166Â°, trunk=83Â°, motion=0.08]
Frame 3: [knee_L=144Â°, knee_R=139Â°, hip_L=167Â°, hip_R=165Â°, trunk=82Â°, motion=0.12]
Frame 4: [knee_L=142Â°, knee_R=137Â°, hip_L=166Â°, hip_R=164Â°, trunk=81Â°, motion=0.15]
         â†‘
         ClasificaciÃ³n: "Walk" (etiqueta del frame 4)

# La ventana se "desliza" un frame hacia adelante:

Frame 1: [knee_L=148Â°, knee_R=143Â°, hip_L=169Â°, hip_R=167Â°, trunk=84Â°, motion=0.05]
Frame 2: [knee_L=146Â°, knee_R=141Â°, hip_L=168Â°, hip_R=166Â°, trunk=83Â°, motion=0.08]
Frame 3: [knee_L=144Â°, knee_R=139Â°, hip_L=167Â°, hip_R=165Â°, trunk=82Â°, motion=0.12]
Frame 4: [knee_L=142Â°, knee_R=137Â°, hip_L=166Â°, hip_R=164Â°, trunk=81Â°, motion=0.15]
Frame 5: [knee_L=140Â°, knee_R=135Â°, hip_L=165Â°, hip_R=163Â°, trunk=80Â°, motion=0.18]
         â†‘
         ClasificaciÃ³n: "Walk" (etiqueta del frame 5)
```

#### 3ï¸âƒ£ Formato del Dataset Final

**Archivo:** `data/processed_windowed/windowed_dataset.csv`

**Estructura:**
```
f_0_knee_left, f_0_knee_right, ..., f_0_motion_energy,  â† Frame mÃ¡s antiguo (t-4)
f_1_knee_left, f_1_knee_right, ..., f_1_motion_energy,  â† Frame t-3
f_2_knee_left, f_2_knee_right, ..., f_2_motion_energy,  â† Frame t-2
f_3_knee_left, f_3_knee_right, ..., f_3_motion_energy,  â† Frame t-1
f_4_knee_left, f_4_knee_right, ..., f_4_motion_energy,  â† Frame actual (t)
label                                                     â† Etiqueta del frame actual
```

**Total:** 30 columnas de features + 1 columna de label = 31 columnas

---

## ğŸ“Š Flujo de Datos Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Video (MP4)    â”‚
â”‚  30 fps, 2 min  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 01_extract_landmarks.py
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Landmarks CSV                  â”‚
â”‚  video, frame, nx_0...nx_32,   â”‚
â”‚                ny_0...ny_32     â”‚
â”‚  (33 puntos Ã— 2 coords = 66)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 02_compute_features.py
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Features CSV (por frame)       â”‚
â”‚  video, frame,                  â”‚
â”‚  knee_L, knee_R, hip_L, hip_R, â”‚
â”‚  trunk_angle, motion_energy     â”‚
â”‚  (6 features)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ + labels (manual)
         â”‚ 04_create_window_dataset.py â­
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Windowed Dataset               â”‚
â”‚  f_0_knee_left, ..., f_4_motion â”‚
â”‚  (5 frames Ã— 6 feat = 30 feat)  â”‚
â”‚  + label                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 05_preprocess_train_split.py
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Train/Test Escalados           â”‚
â”‚  + scaler.pkl (30 features)     â”‚
â”‚  + label_encoder.pkl            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 06_train_models.py
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Modelos Entrenados             â”‚
â”‚  randomforest.pkl               â”‚
â”‚  svm.pkl                        â”‚
â”‚  xgboost.pkl                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Copy to assets/
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ProducciÃ³n (Tiempo Real)       â”‚
â”‚  run_realtime.py                â”‚
â”‚  - Detecta pose (MediaPipe)     â”‚
â”‚  - Calcula 6 features           â”‚
â”‚  - Guarda Ãºltimos 5 frames      â”‚
â”‚  - Aplana ventana (30 feat)     â”‚
â”‚  - Escala con scaler.pkl        â”‚
â”‚  - Predice con modelo           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¬ ImplementaciÃ³n de Sliding Windows

### En Entrenamiento (04_create_window_dataset.py)

```python
WINDOW_SIZE = 5  # NÃºmero de frames de contexto

# Para cada video (NO mezclar videos diferentes):
for video in videos:
    features = video.features  # Array (N_frames, 6)
    labels = video.labels      # Array (N_frames,)
    
    # Iterar desde frame 4 (Ã­ndice donde ya hay 5 frames de historia)
    for i in range(WINDOW_SIZE - 1, len(features)):
        # Extraer ventana: frames [i-4, i-3, i-2, i-1, i]
        window = features[i - (WINDOW_SIZE - 1) : i + 1]
        
        # Aplanar: (5, 6) â†’ (30,)
        window_flat = window.flatten()
        
        # La etiqueta es la del frame ACTUAL (i)
        label = labels[i]
        
        # Guardar muestra: [f0_feat0, f0_feat1, ..., f4_feat5, label]
        dataset.append(window_flat + [label])
```

**Resultado:**
- Frame 0-3: Descartados (no hay suficiente historia)
- Frame 4: Primera muestra (historia completa)
- Frame N: Ãšltima muestra

### En Tiempo Real (run_realtime.py)

```python
from collections import deque

WINDOW_SIZE = 5
features_buffer = deque(maxlen=WINDOW_SIZE)  # Cola FIFO

while True:
    frame = camera.read()
    landmarks = mediapipe.detect(frame)
    
    # Calcular 6 features del frame actual
    current_features = calculate_features(landmarks)
    
    # Agregar a buffer (automÃ¡ticamente elimina el mÃ¡s viejo si lleno)
    features_buffer.append(current_features)
    
    # Solo predecir cuando tenemos 5 frames completos
    if len(features_buffer) == WINDOW_SIZE:
        # Aplanar ventana: (5, 6) â†’ (30,)
        window_flat = np.array(features_buffer).flatten()
        
        # Crear DataFrame para mantener nombres de features
        window_df = pd.DataFrame([window_flat], columns=FEATURE_COLUMNS)
        
        # Escalar
        window_scaled = scaler.transform(window_df)
        
        # Predecir
        prediction = model.predict(window_scaled)
        
        show_on_screen(prediction)
    else:
        show_on_screen("Cargando contexto...")
```

---

## ğŸ”§ Scripts Detallados

### 01_extract_landmarks.py
**Entrada:** Videos MP4 en `data/raw_videos/`  
**Salida:** CSVs en `data/preprocessed/` con 66 columnas (33 landmarks Ã— 2 coords)  
**QuÃ© hace:** Usa MediaPipe Pose para detectar 33 puntos del cuerpo por frame

---

### 02_compute_features.py
**Entrada:** CSVs de landmarks (`data/preprocessed/*_preprocessed.csv`)  
**Salida:** CSVs de features (`data/features_per_frame/*_features.csv`)  
**QuÃ© hace:**
- Calcula Ã¡ngulos de rodillas y caderas
- Calcula inclinaciÃ³n del tronco
- Calcula energÃ­a de movimiento (diferencia entre frames)
- **Resultado:** 6 features por frame

---

### 03_create_labels_csv.py
**Entrada:** Export de LabelStudio o creaciÃ³n manual  
**Salida:** `data/labels/processed/processed_labels.csv`  
**Formato:** `video_name, frame, label`  
**QuÃ© hace:** Convierte anotaciones manuales a formato frame-by-frame

---

### 04_create_window_dataset.py â­
**Entrada:**
- Features: `data/features_per_frame/*_features.csv`
- Labels: `data/labels/processed/processed_labels.csv`

**Salida:** `data/processed_windowed/windowed_dataset.csv`

**QuÃ© hace:**
1. Carga todos los CSVs de features
2. Une features con labels por `(video_name, frame)`
3. Ordena por video y frame
4. **Aplica ventana deslizante:**
   - Por cada video (separadamente)
   - Crea ventanas de 5 frames consecutivos
   - Aplana cada ventana: (5, 6) â†’ (30,)
   - Asigna la etiqueta del frame actual
5. Concatena todas las ventanas en un solo dataset

**ParÃ¡metros clave:**
```python
WINDOW_SIZE = 5  # Modificar aquÃ­ para cambiar contexto temporal
```

---

### 05_preprocess_train_split.py
**Entrada:** `data/processed_windowed/windowed_dataset.csv`  
**Salida:**
- `data/processed_windowed/train_dataset.csv`
- `data/processed_windowed/test_dataset.csv`
- `data/processed_windowed/scaler.pkl` â­ (entrena con 30 features)
- `data/processed_windowed/label_encoder.pkl`

**QuÃ© hace:**
1. Separa features (30 cols) y labels (1 col)
2. Split train/test (80/20, estratificado)
3. **Balancea clases** con SMOTE (solo train)
4. **Entrena StandardScaler** con train (30 features)
5. Escala train y test con el mismo scaler
6. Codifica labels (texto â†’ nÃºmeros)

---

### 06_train_models.py
**Entrada:**
- `data/processed_windowed/train_dataset.csv`
- `data/processed_windowed/test_dataset.csv`

**Salida:**
- `results/models/randomforest.pkl`
- `results/models/svm.pkl`
- `results/models/xgboost.pkl`
- `results/metrics/model_comparison.csv`

**QuÃ© hace:**
1. Entrena 3 modelos con validaciÃ³n cruzada (5-fold)
2. EvalÃºa en conjunto de test
3. Guarda mÃ©tricas y reportes

---

### run_realtime.py â­
**Entrada:**
- Webcam en vivo
- `assets/randomforest.pkl`
- `assets/scaler.pkl` (30 features)
- `assets/label_encoder.pkl`

**Salida:** PredicciÃ³n en tiempo real en pantalla

**QuÃ© hace:**
1. Captura frames de webcam
2. Detecta pose con MediaPipe
3. Calcula 6 features por frame
4. **Mantiene buffer de 5 frames** (deque)
5. Cuando buffer estÃ¡ lleno:
   - Aplana ventana (30 features)
   - Escala con scaler
   - Predice con modelo
   - Muestra actividad en pantalla

---

## âš™ï¸ ParÃ¡metros Configurables

### WINDOW_SIZE (TamaÃ±o de Ventana)

**Ubicaciones:**
- `04_create_window_dataset.py` lÃ­nea ~15
- `run_realtime.py` lÃ­nea ~28

**Valores comunes:**
- `WINDOW_SIZE = 3`: Menos contexto, mÃ¡s rÃ¡pido, menos preciso
- `WINDOW_SIZE = 5`: âœ… **Recomendado** (balance)
- `WINDOW_SIZE = 7`: MÃ¡s contexto, mÃ¡s preciso, menos muestras
- `WINDOW_SIZE = 10`: Mucho contexto, lento en tiempo real

**âš ï¸ IMPORTANTE:** Si cambias WINDOW_SIZE, debes:
1. Volver a ejecutar `04_create_window_dataset.py`
2. Volver a ejecutar `05_preprocess_train_split.py` (nuevo scaler con NÃ—6 features)
3. Volver a ejecutar `06_train_models.py`
4. Actualizar `run_realtime.py` con el mismo valor

---

## ğŸš€ Inicio RÃ¡pido

```bash
# 1. Clonar repositorio
cd "Entrega 3"

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Colocar datos:
#    - Landmarks CSV en: data/preprocessed/
#    - Labels CSV en: data/labels/processed/processed_labels.csv

# 4. Ejecutar pipeline completo
python run_full_pipeline.py

# 5. Verificar resultados
cat results/metrics/model_comparison.csv

# 6. Ejecutar tiempo real
python 4_real_time_app/run_realtime.py
```

---

## ğŸ“ Conceptos Clave para Nuevos Colaboradores

### 1. Â¿Por quÃ© 30 features?
```
6 features/frame Ã— 5 frames = 30 features totales
```

### 2. Â¿Por quÃ© no mezclar videos en las ventanas?
```python
# âŒ MAL: Video1_frame99, Video2_frame0, ... â†’ ventana invÃ¡lida
# âœ… BIEN: Video1_frame4-8 â†’ ventana vÃ¡lida
```
Cada video se procesa independientemente para evitar discontinuidades.

### 3. Â¿QuÃ© hace el scaler?
Normaliza las 30 features para que todas estÃ©n en la misma escala (media=0, std=1).
**Crucial:** El scaler de entrenamiento DEBE ser el mismo en tiempo real.

### 4. Â¿CÃ³mo funciona la cola (deque)?
```python
deque(maxlen=5)  # FIFO (First In, First Out)
[A, B, C, D, E]  # Lleno
append(F)
[B, C, D, E, F]  # A se eliminÃ³ automÃ¡ticamente
```

---

## ğŸ“š Recursos Adicionales

- **MediaPipe Pose:** https://google.github.io/mediapipe/solutions/pose
- **Sliding Windows en Time Series:** https://machinelearningmastery.com/time-series-forecasting-supervised-learning/
- **DocumentaciÃ³n scikit-learn:** https://scikit-learn.org/

---

**âœ… Â¡Listo para empezar!**
